{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Vector Representations\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
    "## 1) *Optional:* Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
    "\n",
    "At a minimum your final dataframe of job listings should contain\n",
    "- Job Title\n",
    "- Job Description\n",
    "\n",
    "If you choose to not to scrape the data, there is a CSV with outdated data in the directory. Remeber, if you scrape Indeed, you're helping yourself find a job. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcYlc1URXhlC"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
    "## 2) Use Spacy to tokenize / clean the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>description</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n      <td>Data scientist</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n      <td>Data Scientist I</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n      <td>Data Scientist - Entry Level</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n      <td>Data Scientist</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n      <td>Data Scientist</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   Unnamed: 0                                        description  \\\n0           0  b\"<div><div>Job Requirements:</div><ul><li><p>...   \n1           1  b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...   \n2           2  b'<div><p>As a Data Scientist you will be work...   \n3           3  b'<div class=\"jobsearch-JobMetadataHeader icl-...   \n4           4  b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...   \n\n                          title  \n0               Data scientist   \n1              Data Scientist I  \n2  Data Scientist - Entry Level  \n3                Data Scientist  \n4                Data Scientist  "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('module2-vector-representations/data/job_listings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htmlParse(theData):\n",
    "    x = BeautifulSoup(theData).get_text()\n",
    "    x = x.replace(\"\\\\n\", \" \")\n",
    "    x = x.replace(\"\\\\xe2\\\\x80\\\\x99\", \"'\")\n",
    "    return x[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description']=df['description'].apply(htmlParse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"As Spotify Premium swells to over 96M subscribers around the globe, we are looking for new ways to continue to grow our subscription business. You would be joining Spotify on the Premium Analytics team, a core business strategy and insights team, as an Associate Data Scientist. In this unique position, your work would be essential in shaping how Spotify is able to grow through data-driven recommendations, new product offerings and innovative marketing efforts. You will see first hand how your work translates into new strategies, products, and consumer experiences as we enter a new phase in Spotify Premium's life.  You will work with a global team of world-class analysts, data scientists, business managers, marketers, and engineers. We are all passionate about what we do and move forward with high impact projects at a high pace. Learning and improving is part of our daily routine, and you will be free to develop your own skills and ways of working. At your fingertips you'll have access to petabytes of data, and will get the opportunity to be creative with how you drive insights and strategies from that. Above all, your work will impact the way the world experiences music.  What You'll Do  Develop data-driven strategies to drive the growth of Spotify subscribers Create and communicate actionable recommendations that improve our product conversion and migration metrics. Work with everything from advanced algorithmic data analysis and AB-test setup to business analysis and modeling Work closely with business stakeholders to understand the change they are driving and help them discover new opportunities for growth. Who Are You  You are an open-minded, creative person with an interest in analyses and data science You have some professional experience working with data analysis (~1 year) You are comfortable with the whole analytical process from identifying insight gaps to designing and running initiatives to fill them You have worked hands-on synthesizing insights from data using tools such as Python, R, SQL, SAS, SPSS, Minitab and/or Hadoop You are a communicative person that values building strong relationships with colleagues and partners, you are experienced in presenting insights and recommendations to partners or clients'\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Data Scientist                                                                     150\nSenior Data Scientist                                                               14\nJunior Data Scientist                                                               10\nAssociate Data Scientist                                                             8\nData Scientist Intern                                                                7\nPrincipal Data Scientist                                                             6\nSr. Data Scientist                                                                   6\nDATA SCIENTIST                                                                       5\nJr. Data Scientist                                                                   4\nData Scientist Internship                                                            4\nData Scientist, Product                                                              4\nData Scientist II                                                                    4\nJunior Data Scientist - Big Data (Entry-Level)                                       3\nData Scientist, Operations Data Science                                              3\nStaff Data Scientist, Vudu                                                           3\nData Scientist - Machine Learning, AdTech                                            2\n2019 University Graduate - Data Scientist - Policy Economics                         2\nData Scientist, Sales                                                                2\nSr Data Scientist, NLP - Customer Obsession                                          2\nData Scientist (GEC11901)                                                            2\nData Science Intern                                                                  2\nClinical Data Scientist                                                              2\nStaff Data Scientist - NLP                                                           2\nGeospatial Data Scientist                                                            2\nInvestigative Data Scientist                                                         2\nStaff Data Scientist                                                                 2\nData Scientist Summer Internship 2019                                                2\nData Science Internship – Summer 2019                                                2\nData Scientist Internship - Summer 2019                                              2\nData Scientist 2                                                                     2\n                                                                                  ... \nData Scientist - Entry Level                                                         1\nData Scientist, Alexa Shopping NLU                                                   1\nData Scientist- Corporate                                                            1\nData Scientist, Infrastructure                                                       1\nData Scientist/Data Analytics Intern - Summer 2019                                   1\nData Scientist-Global People Analytics                                               1\nSenior Business Intelligence Data Engineer                                           1\nSenior Data Science Engineer                                                         1\nData Scientist Intern - Summer 2019                                                  1\nSenior Data Scientist - Modeling                                                     1\nIntern, Data Science                                                                 1\nSenior Data Scientist, Customer Experience                                           1\nCCS Data Scientist                                                                   1\nData Scientist - SEA                                                                 1\nIBM Marketing PhD Data Scientist Intern, Summer 2019                                 1\nData Engineer                                                                        1\nData scientist                                                                       1\nIntern, Data Engineer                                                                1\nAnalyst, Data Scientist                                                              1\nData Scientist - Styling Algorithms                                                  1\n2019 PhD Data Scientist Internship - Forecasting and Anomaly Detection Platform      1\nData Scientist - Forecasting and Anomaly Detection Platform                          1\nData Scientist I-III                                                                 1\nData Scientist (PYTHON, HADOOP)                                                      1\nData Scientist- Zillow Offers                                                        1\nData Scientist (Intern)                                                              1\nData Scientist, Advanced Marketing Analytics                                         1\nData Scientist, Pricing                                                              1\nData Scientist (Outward, Inc.)                                                       1\nApplied Data Scientist                                                               1\nName: title, Length: 177, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "\"\"\" Make them tokens \"\"\"\n",
    "for doc in tokenizer.pipe(df['description'], batch_size=500):\n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) & (token.is_punct == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "\n",
    "    tokens.append(doc_tokens)\n",
    "\n",
    "df['description_token'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    Job Requirements: Conceptual understanding in ...\n1    Job Description  As a Data Scientist 1, you wi...\n2    As a Data Scientist you will be working on con...\n3    $4,969 - $6,756 a monthContractUnder the gener...\n4    Location: USA \\xe2\\x80\\x93 multiple locations ...\nName: description, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
    "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit((df['description']))\n",
    "dtm = vect.transform(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "nsitivity', 'sensor', 'sensors', 'sensory', 'sentiment', 'seo', 'separated', 'separating', 'sequence', 'sequencing', 'sequential', 'serializing', 'series', 'serious', 'seriously', 'serta', 'serve', 'served', 'serveng', 'server', 'servers', 'serves', 'service', 'servicenow', 'services', 'servicing', 'serving', 'sessions', 'set', 'sets', 'setsprovide', 'setting', 'settings', 'settle', 'setup', 'seven', 'seventh', 'several', 'sex', 'sexual', 'sf', 'sf1', 'sfarm', 'sfdc', 'shake', 'shaking', 'shape', 'shapes', 'shaping', 'share', 'shared', 'shareholders', 'sharepoint', 'shares', 'sharing', 'sharp', 'sharpen', 'sharpening', 'shaw', 'she', 'sheets', 'shell', 'shepherding', 'shield', 'shift', 'shining', 'shiny', 'ship', 'shipped', 'shipping', 'ships', 'shirt', 'shirts', 'shocks', 'shogun', 'shooting', 'shop', 'shopper', 'shoppers', 'shopping', 'shops', 'shorelines', 'shorenstein', 'short', 'shortlist', 'shorts', 'shot', 'should', 'shoulders', 'shouldn', 'show', 'showcase', 'shower', 'showing', 'shown', 'shows', 'shy', 'siamese', 'sick', 'side', 'sided', 'sierra', 'sifts', 'sightseeing', 'sigir', 'sign', 'signal', 'signalfire', 'signals', 'signed', 'significance', 'significanceexperience', 'significanceprovide', 'significant', 'significantly', 'signs', 'silicon', 'silos', 'silverrail', 'similar', 'similarities', 'similarity', 'similarly', 'similiar', 'simmons', 'simple', 'simplerleading', 'simplest', 'simplicity', 'simplified', 'simplify', 'simplifying', 'simply', 'simulate', 'simulation', 'simulations', 'simultaneous', 'simultaneousl', 'simultaneously', 'since', 'sincere', 'sincerely', 'singapore', 'single', 'singular', 'singularly', 'sisense', 'sister', 'sit', 'site', 'sites', 'sits', 'sitting', 'situation', 'situations', 'six', 'sixleaf', 'sizable', 'size', 'sizeable', 'sized', 'sizes', 'sizing', 'skepticism', 'skill', 'skilled', 'skillful', 'skills', 'skillset', 'skillsexcellent', 'skillsproficiency', 'skillsstrong', 'skillssuperior', 'sklearn', 'sky', 'skype', 'skyrocketed', 'sl', 'slack', 'sleep', 'sleeptracker', 'sleeves', 'sleuth', 'slide', 'slides', 'slowing', 'slq', 'slync', 'sm', 'small', 'smaller', 'smart', 'smartbed', 'smarter', 'smartest', 'smartview', 'sme', 'smes', 'smile', 'smiles', 'smoke', 'smooth', 'smoothly', 'sms', 'sn1', 'snacks', 'snag', 'snapchat', 'snc', 'sne', 'snowflake', 'so', 'soak', 'soap', 'social', 'socialize', 'socializing', 'socials', 'societal', 'societas', 'societies', 'society', 'socioeconomic', 'sociology', 'sociotechnical', 'sockeye', 'socure', 'sofia', 'soft', 'softbank', 'softly', 'softvision', 'software', 'soho', 'soil', 'sojern', 'sold', 'sole', 'solely', 'solicit', 'solid', 'solidifying', 'solo', 'solopreneurs', 'solr', 'solution', 'solutioning', 'solutions', 'solutionsdemonstrated', 'solvable', 'solve', 'solved', 'solver', 'solvers', 'solves', 'solving', 'som', 'some', 'somebody', 'someone', 'something', 'sometimes', 'sonoma', 'sony', 'soon', 'sophisticated', 'sophistication', 'sops', 'sorry', 'sought', 'sound', 'soundcloud', 'sounds', 'source', 'sourced', 'sources', 'sourcescleanses', 'sourcessupport', 'sourcing', 'south', 'southeast', 'space', 'spaceflight', 'spaces', 'spacy', 'spam', 'span', 'spanish', 'spanning', 'spans', 'sparing', 'spark', 'sparkexperience', 'sparkfund', 'sparkml', 'sparkr', 'sparql', 'sparse', 'spatial', 'speak', 'speaker', 'speaking', 'speaks', 'spearhead', 'special', 'specialist', 'specialists', 'specialities', 'specialization', 'specializations', 'specialize', 'specialized', 'specializes', 'specializing', 'specially', 'specialty', 'specific', 'specifically', 'specification', 'specifications', 'specified', 'specify', 'specifying', 'specs', 'spectral', 'spectrum', 'speech', 'speed', 'spend', 'spending', 'spends', 'spent', 'speyer', 'spglobal', 'spicy', 'spin', 'spinoff', 'spirit', 'spirt', 'split', 'splunk', 'spoc', 'spoken', 'spokesperson', 'sponsor', 'sponsored', 'sponsoring', 'sponsors', 'sponsorship', 'sport', 'sporting', 'sports', 'spot', 'spotfire', 'spotify', 'spreading', 'spreadsheet', 'spring', 'springfield', 'springs', 'sprint', 'sps', 'spss', 'sql', 'sqlproficient', 'sqoop', 'square', 'squarely', 'squares', 'squarespace', 'squareup', 'sr', 'srilm', 'ssa', 'ssas', 'ssh', 'ssis', 'ssu', 'st', 'stability', 'stable', 'stack', 'stacked', 'stackoverflow', 'stacks', 'stadium', 'staff', 'staffed', 'staffing', 'stage', 'stages', 'stake', 'stakeholder', 'stakeholders', 'stalking', 'stan', 'stand', 'standalone', 'standard', 'standardize', 'standardized', 'standards', 'standing', 'standpoint', 'stanford', 'stanley', 'stanza', 'staples', 'star', 'stardog', 'start', 'started', 'starter', 'starters', 'starting', 'starts', 'startup', 'startups', 'stat', 'stata', 'state', 'stated', 'statement', 'statements', 'states', 'statewide', 'static', 'station', 'stationless', 'stations', 'statistic', 'statistica', 'statistical', 'statistically', 'statisticians', 'statistics', 'stats', 'statsmodels', 'status', 'statusfull', 'stay', 'staying', 'stays', 'steaks', 'stellar', 'stem', 'step', 'stephanie', 'steps', 'stereotypes', 'steward', 'stewarding', 'stewardship', 'sticks', 'still', 'stimulating', 'stipend', 'stitch', 'stitching', 'stochastic', 'stock', 'stocked', 'stone', 'stoop', 'stop', 'storage', 'storages', 'store', 'stored', 'stores', 'stories', 'storing', 'storm', 'story', 'storycapacity', 'storyteller', 'storytelling', 'straight', 'strain', 'strategic', 'strategically', 'strategics', 'strategies', 'strategiesdevelop', 'strategiesdocumenting', 'strategists', 'strategize', 'strategizes', 'strategy', 'stream', 'streaming', 'streamline', 'streamlined', 'streamlines', 'streams', 'streeam', 'street', 'streets', 'strength', 'strengthen', 'strengthening', 'strengths', 'stress', 'stretch', 'strict', 'strides', 'strings', 'strive', 'strives', 'striving', 'strong', 'stronger', 'strongest', 'strongly', 'structural', 'structure', 'structured', 'structures', 'structuring', 'struggle', 'student', 'students', 'studies', 'studio', 'studios', 'study', 'studying', 'stuff', 'stunning', 'stupid', 'style', 'styles', 'styling', 'stylish', 'stylist', 'stylists', 'sub', 'subcontractor', 'subcontracts', 'subcultures', 'subgroups', 'subject', 'subjective', 'submission', 'submissions', 'submit', 'submitted', 'subordinate', 'subordinates', 'subscriber', 'subscribers', 'subscription', 'subscriptions', 'subsequent', 'subsequently', 'subsidiaries', 'subsidiary', 'substance', 'substances', 'substantial', 'substantive', 'substitute', 'substituted', 'subsurface', 'subtle', 'suburban', 'succeed', 'succeeding', 'success', 'successes', 'successful', 'successfully', 'succinctly', 'such', 'sudden', 'suffering', 'sufficient', 'sugar', 'suggest', 'suggestion', 'suggestions', 'suitability', 'suitable', 'suite', 'suited', 'suites', 'sum', 'summaries', 'summarization', 'summarizations', 'summarize', 'summarizes', 'summarizing', 'summary', 'summer', 'summit', 'sunny', 'sunnyvale', 'superb', 'superficially', 'superior', 'supervise', 'supervised', 'supervision', 'supervisor', 'supervisors', 'supervisory', 'supplement', 'supplementary', 'suppliers', 'supplies', 'supply', 'support', 'supported', 'supporting', 'supportive', 'supports', 'sure', 'surface', 'surfaces', 'surfacing', 'surgeons', 'surgeries', 'surgery', 'surgical', 'surprised', 'surprising', 'surrounded', 'surrounding', 'surroundings', 'survey', 'surveys', 'survival', 'sustain', 'sustainability', 'sustainable', 'sustainably', 'sustained', 'svb', 'svm', 'svms', 'svn', 'svp', 'sw', 'swagger', 'swapnil', 'sweat', 'sweden', 'swells', 'swift', 'switch', 'switching', 'swoodoo', 'sycamore', 'sydney', 'symbol', 'sync', 'syndicated', 'synergies', 'synergy', 'syntactic', 'synthesis', 'synthesize', 'synthesized', 'synthesizes', 'synthesizing', 'synthetic', 'system', 'systematic', 'systematically', 'systemic', 'systems', 'systemsbuild', 'systemsproficiency', 't2t', 'table', 'tableau', 'tables', 'tabulate', 'tackle', 'tackles', 'tackling', 'taco', 'tactic', 'tactical', 'tactically', 'tactics', 'tag', 'tagging', 'tagline', 'tail', 'tailor', 'tailored', 'take', 'takecareof', 'taken', 'takes', 'taking', 'talent', 'talentacquisition', 'talented', 'talents', 'talk', 'talking', 'talks', 'talksa', 'tangible', 'tank', 'tap', 'tapingo', 'tapping', 'target', 'targeted', 'targeting', 'task', 'tasked', 'tasking', 'tasks', 'taught', 'tax', 'taxa', 'taxes', 'taxonomy', 'tb', 'tcs', 'teach', 'teacher', 'teachers', 'teaching', 'team', 'teamed', 'teamextremely', 'teammate', 'teammates', 'teams', 'teamsexperience', 'teamsto', 'teamstrong', 'teamwork', 'tech', 'techcrunch', 'technical', 'technically', 'technics', 'technique', 'techniques', 'techniquesrecommend', 'technological', 'technologically', 'technologies', 'technologiesstay', 'technologist', 'technologists', 'technology', 'technologymanage', 'telco', 'telecom', 'telecommunication', 'telecommunications', 'telecommute', 'telecommuter', 'telecommuters', 'telecommuting', 'telemetry', 'telephone', 'television', 'telework', 'tell', 'telling', 'tells', 'temp', 'tempe', 'templates', 'temporal', 'temporary', 'ten', 'tennis', 'tens', 'tensor', 'tensorflow', 'tenure', 'terabyte', 'terabytes', 'teradata', 'teradataexcellent', 'term', 'termimpact', 'terminal', 'termination', 'terminology', 'terminologyability', 'terminologyworks', 'terms', 'territories', 'territory', 'test', 'testable', 'tested', 'testing', 'tests', 'texas', 'text', 'textbooks', 'textio', 'texts', 'textual', 'tf', 'th', 'than', 'thank', 'thanks', 'that', 'the', 'theano', 'theft', 'their', 'them', 'theme', 'themselves', 'themwe', 'then', 'theorem', 'theoretical', 'theories', 'theorist', 'theory', 'therapeutic', 'therapeutics', 'therapies', 'therapist', 'there', 'thereafter', 'thereby', 'therefore', 'these', 'thesis', 'they', 'thing', 'things', 'think', 'thinker', 'thinkers', 'thinkerswe', 'thinking', 'thinks', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'thoughtful', 'thousand', 'thousands', 'threat', 'threats', 'three', 'thresholds', 'thrill', 'thrilled', 'thrilling', 'thrive', 'thrives', 'thriving', 'through', 'throughout', 'throughput', 'thru', 'thursday', 'thus', 'tibco', 'tick', 'ticker', 'ticket', 'tickets', 'tide', 'tidy', 'tidyr', 'tidyverse', 'tie', 'tied', 'tier', 'ties', 'tight', 'tightly', 'time', 'timeabout', 'timecontractor', 'timeframe', 'timeline', 'timelines', 'timeliness', 'timely', 'timeqxbranch', 'times', 'timethe', 'tinker', 'tinton', 'tiny', 'tips', 'tirelessly', 'tishman', 'titan', 'title', 'tl1', 'tm', 'tmus', 'tn', 'to', 'tobacco', 'today', 'together', 'tokenization', 'tokyo', 'tolerance', 'tolerate', 'tolerated', 'tomorrow', 'tonnes', 'tons', 'too', 'took', 'tool', 'toolbox', 'toolboxes', 'tooling', 'toolkit', 'toolkits', 'tools', 'toolset', 'toolssolid', 'toomey', 'top', 'topic', 'topics', 'topline', 'torch', 'total', 'totaling', 'touch', 'touches', 'touching', 'touchpoint', 'touchpoints', 'tough', 'toughest', 'tour', 'tournaments', 'tours', 'toward', 'towards', 'towers', 'town', 'toxic', 'toyota', 'track', 'tracked', 'tracking', 'traction', 'trade', 'traded', 'trademark', 'trademarks', 'tradeoffs', 'trading', 'traditional', 'traditionally', 'traffic', 'trail', 'trailblazers', 'train', 'trained', 'training', 'trainingexecuting', 'trainings', 'traits', 'transacting', 'transaction', 'transactional', 'transactions', 'transfer', 'transferable', 'transferring', 'transfers', 'transform', 'transformation', 'transformational', 'transformations', 'transformationsas', 'transformative', 'transforming', 'transforms', 'transgender', 'transit', 'transition', 'transitioning', 'translate', 'translates', 'translating', 'translation', 'transliteration', 'transmission', 'transparency', 'transparent', 'transparently', 'transportation', 'travel', 'traveldoo', 'traveler', 'travelers', 'traveling', 'travelocity', 'travelu', 'traversing', 'treading', 'treat', 'treated', 'treating', 'treatment', 'treatments', 'tree', 'trees', 'tremendous', 'tremendously', 'trend', 'trending', 'trends', 'trendsanalyzing', 'tri', 'triage', 'trial', 'trials', 'tribe', 'tries', 'triggers', 'trillion', 'trilogy', 'trip', 'triple', 'trips', 'trivago', 'trivally', 'trivial', 'trouble', 'troubleshoot', 'troubleshooting', 'troves', 'truck', 'trucking', 'trucks', 'true', 'truly', 'trust', 'trusted', 'trusting', 'truth', 'try', 'trying', 'ts', 'tsql', 'tuesday', 'tuition', 'tune', 'tuning', 'turn', 'turnaround', 'turned', 'turning', 'turns', 'tv', 'tvs', 'tweak', 'twelve', 'twenty', 'twice', 'twilio', 'twitter', 'two', 'tx', 'type', 'types', 'typescript', 'typical', 'typically', 'uber', 'uberpool', 'uberx', 'uc', 'udacity', 'ui', 'uild', 'ukraine', 'ulta', 'ultimate', 'ultimately', 'un', 'unable', 'unaided', 'unanticipated', 'unapproved', 'unasked', 'unassailable', 'unauthorized', 'unavailable', 'unbelievably', 'unbiased', 'unbounded', 'uncertainty', 'uncharted', 'uncleansed', 'uncommon', 'uncommongoods', 'unconventional', 'uncover', 'uncovering', 'uncovers', 'undefined', 'under', 'underbelly', 'undergoing', 'undergradinternships', 'undergraduate', 'underlies', 'underlying', 'underneath', 'underperforming', 'underpin', 'underpinnings', 'underrepresented', 'understand', 'understandable', 'understanding', 'understands', 'understood', 'undertake', 'undertaken', 'undertaking', 'underwriting', 'undue', 'unearth', 'unemployment', 'unexpected', 'unfamiliar', 'unfurnished', 'unicorns', 'unified', 'uniformed', 'uniformity', 'unify', 'unifying', 'unimagined', 'union', 'unions', 'unique', 'uniquely', 'uniqueness', 'unison', 'unit', 'united', 'unitedhealth', 'unitedhealthcare', 'unites', 'units', 'univariate', 'universal', 'universally', 'universe', 'universities', 'university', 'unix', 'unknown', 'unknowns', 'unl', 'unlawful', 'unleashing', 'unless', 'unlikely', 'unlimited', 'unlock', 'unlocking', 'unlocks', 'unmatched', 'unmet', 'unnecessary', 'unparalleled', 'unprecedented', 'unpredictable', 'unquestioned', 'unrelated', 'unrivaled', 'unsecured', 'unseen', 'unsolicited', 'unsolved', 'unstructured', 'unsupervised', 'untapped', 'until', 'unusually', 'unwavering', 'unwilling', 'unwind', 'up', 'upbeat', 'upcoming', 'update', 'updated', 'updates', 'updating', 'upfront', 'upgrade', 'upgrades', 'upholding', 'uploaded', 'upon', 'upper', 'upside', 'upstream', 'uptick', 'urban', 'urgency', 'urgent', 'urgently', 'urt1', 'us', 'us_employment_compliance', 'usa', 'usaa', 'usable', 'usage', 'usd', 'use', 'used', 'useful', 'usemploymentcompliance', 'user', 'users', 'uses', 'usher', 'usinfosys', 'using', 'usingprogramming', 'usstratcom', 'ustranscom', 'usually', 'utah', 'utica', 'utilities', 'utility', 'utilization', 'utilize', 'utilized', 'utilizes', 'utilizing', 'utilizingapplicable', 'utm_medium', 'utm_source', 'ux', 'v1', 'va', 'vacancies', 'vacation', 'vacations', 'vague', 'valid', 'validate', 'validated', 'validating', 'validation', 'validationexperience', 'validity', 'valley', 'valuable', 'value', 'valued', 'values', 'valuing', 'vanishing', 'variable', 'variables', 'variance', 'variancecollaborate', 'variation', 'variational', 'varied', 'varies', 'variety', 'various', 'vary', 'varying', 'vast', 'vastly', 'vba', 'vc', 'vcs', 've', 'vector', 'vectorizing', 'vectors', 'vehicle', 'vehicles', 'velocity', 'vendor', 'vendors', 'venrock', 'venture', 'ventures', 'verb', 'verbal', 'verbally', 'verification', 'verified', 'verifies', 'verify', 'verifying', 'verily', 'verizon', 'vermont', 'versa', 'versant', 'versatile', 'versatility', 'versed', 'version', 'versioning', 'versus', 'vertica', 'vertical', 'verticals', 'very', 'vet', 'veteran', 'veterans', 'vets', 'vh1', 'via', 'viability', 'viable', 'viacom', 'viagogo', 'vibrant', 'vice', 'victim', 'victories', 'video', 'videoiq', 'videos', 'view', 'viewability', 'viewable', 'viewer', 'viewers', 'viewing', 'viewpoints', 'views', 'vigilant', 'village', 'violate', 'violations', 'violence', 'virginia', 'virtual', 'virtualization', 'virtually', 'virtue', 'virtues', 'viruses', 'visa', 'visas', 'visibility', 'visible', 'vision', 'visionaries', 'visionary', 'visionpush', 'visit', 'visitation', 'visiting', 'visits', 'visual', 'visualisations', 'visualization', 'visualizations', 'visualizationsexperience', 'visualizationstrong', 'visualize', 'visualizes', 'visualizing', 'visually', 'visuals', 'vital', 'vitamin', 'vitamins', 'vizio', 'vlookup', 'vms', 'vmware', 'voice', 'voices', 'voiding', 'volume', 'volumes', 'volunteer', 'volunteering', 'volunteerism', 'vosb', 'voscal', 'vowpal', 'vp', 'vps', 'vr', 'vs', 'vudu', 'w2', 'wa', 'wabbit', 'wacker', 'wages', 'wagging', 'wait', 'waiting', 'waitr', 'waitrapp', 'wakefield', 'walk', 'walking', 'wall', 'walled', 'wallet', 'walls', 'walmart', 'walmartlabs', 'want', 'wanted', 'wants', 'warehouse', 'warehouses', 'warehousing', 'warehousingdata', 'warn', 'warner', 'warning', 'warrant', 'warranties', 'warranty', 'warrior', 'was', 'washington', 'waste', 'water', 'waterfall', 'waterfront', 'waters', 'watershed', 'watson', 'waukesha', 'wave', 'way', 'ways', 'wdatpeppres', 'wdatpred', 'we', 'weaklings', 'wealth', 'wear', 'wearable', 'weather', 'web', 'webrtc', 'webscraping', 'website', 'websites', 'websockets', 'wednesday', 'wednesdays', 'week', 'weekday', 'weekends', 'weekly', 'weeks', 'weighted', 'weka', 'welcome', 'welcomed', 'welcomes', 'welcoming', 'welding', 'well', 'wellbeing', 'wellness', 'were', 'west', 'westlake', 'westrock', 'wetlands', 'wework', 'wfh', 'wfs', 'what', 'whatever', 'whatsapp', 'whd', 'when', 'whenever', 'where', 'wherever', 'whether', 'which', 'while', 'whilst', 'white', 'whiz', 'who', 'whole', 'wholesome', 'wholly', 'whom', 'whose', 'why', 'wide', 'widely', 'wider', 'widespread', 'wild', 'wildlife', 'will', 'williams', 'willing', 'willingness', 'win', 'windowing', 'windows', 'winners', 'winning', 'wipro', 'wire', 'wireless', 'wireline', 'wisconsin', 'wisdom', 'wish', 'with', 'withdraw', 'within', 'without', 'wizard', 'wm', 'wolferman', 'women', 'won', 'worcester', 'word', 'word2vec', 'words', 'work', 'workday', 'worked', 'worker', 'workers', 'workersdevelop', 'workflow', 'workflows', 'workforce', 'working', 'workings', 'workload', 'workplace', 'works', 'workshops', 'workspace', 'world', 'worldgrid', 'worldline', 'worldwide', 'worth', 'wotif', 'would', 'wrangle', 'wrangler', 'wrangles', 'wrangling', 'wrapped', 'write', 'writers', 'writing', 'written', 'writtenproven', 'wrk', 'wwe', 'www', 'x80', 'x81ciency', 'x81eld', 'x81nd', 'x81ndings', 'x81table', 'x82', 'x83', 'x84', 'x8bthis', 'x8bwe', 'x90if', 'x90paced', 'x93', 'x93august', 'x93minorities', 'x94', 'x9414', 'x94a', 'x94and', 'x94by', 'x94combined', 'x94cryptocurrencies', 'x94e', 'x94have', 'x94how', 'x94including', 'x94is', 'x94it', 'x94no', 'x94not', 'x94stands', 'x94such', 'x94supporting', 'x94the', 'x94to', 'x94typically', 'x94unlocking', 'x94we', 'x94you', 'x98', 'x98big', 'x98own', 'x98real', 'x98think', 'x9c', 'x9cbang', 'x9cbest', 'x9cbig', 'x9cchristmas', 'x9ccustomer', 'x9cget', 'x9chave', 'x9chr', 'x9cinnovate', 'x9clive', 'x9cmachine', 'x9cmake', 'x9cpassion', 'x9cpeople', 'x9cplant', 'x9cpragmatism', 'x9cprovide', 'x9cquality', 'x9creasonable', 'x9crivr', 'x9cscientific', 'x9cscorecards', 'x9cstem', 'x9cstorytelling', 'x9csubject', 'x9csurge', 'x9cteam', 'x9cthe', 'x9ctop', 'x9ctraditional', 'x9cvirtual', 'x9cwe', 'x9cwhole', 'x9cwhy', 'x9cwin', 'x9cwork', 'x9cwow', 'x9cwowing', 'x9d', 'xa0', 'xa2', 'xa6', 'xa6and', 'xa6apple', 'xa6curious', 'xa6protect', 'xa6rapidly', 'xa7', 'xa8ve', 'xa9', 'xa9al', 'xa9cor', 'xac', 'xae', 'xafve', 'xb7', 'xb7experience', 'xb7identify', 'xb7knowledge', 'xbb', 'xbf', 'xbox', 'xc2', 'xc3', 'xe2', 'xef', 'xgboost', 'xp', 'xpo', 'yards', 'year', 'yeara', 'yearas', 'yearcollects', 'yeardescription', 'yearjob', 'yearlrs', 'years', 'yearsexperience', 'yearsummary', 'yearthe', 'yeartitle', 'yearworking', 'yes', 'yet', 'yeti', 'yield', 'york', 'you', 'young', 'your', 'yours', 'yourself', 'youtube', 'yrs', 'zenreach', 'zero', 'zeus', 'zf', 'zheng', 'zillow', 'zogsports', 'zones', 'zoom', 'zuckerberg', 'zurich']\n"
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(0, 230)\t2\n  (0, 509)\t1\n  (0, 580)\t2\n  (0, 620)\t2\n  (0, 648)\t1\n  (0, 674)\t1\n  (0, 688)\t1\n  (0, 710)\t1\n  (0, 769)\t1\n  (0, 932)\t1\n  (0, 1090)\t1\n  (0, 1493)\t1\n  (0, 1551)\t1\n  (0, 1635)\t1\n  (0, 1713)\t1\n  (0, 1724)\t1\n  (0, 2101)\t1\n  (0, 2162)\t1\n  (0, 2197)\t1\n  (0, 2522)\t1\n  (0, 2659)\t1\n  (0, 2893)\t1\n  (0, 3007)\t2\n  (0, 3024)\t1\n  (0, 3055)\t1\n  :\t:\n  (425, 8368)\t1\n  (425, 8374)\t1\n  (425, 8395)\t1\n  (425, 8444)\t1\n  (425, 8468)\t2\n  (425, 8505)\t1\n  (425, 8511)\t1\n  (425, 8513)\t1\n  (425, 8519)\t2\n  (425, 8532)\t3\n  (425, 8534)\t2\n  (425, 8548)\t10\n  (425, 8550)\t2\n  (425, 8551)\t1\n  (425, 8561)\t7\n  (425, 8567)\t1\n  (425, 8570)\t2\n  (425, 8573)\t1\n  (425, 8597)\t2\n  (425, 8610)\t2\n  (425, 8707)\t2\n  (425, 8720)\t2\n  (425, 8726)\t1\n  (425, 8731)\t3\n  (425, 8733)\t1\n"
    }
   ],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>02115</th>\n      <th>03</th>\n      <th>0305</th>\n      <th>0356</th>\n      <th>04</th>\n      <th>062</th>\n      <th>06366</th>\n      <th>08</th>\n      <th>...</th>\n      <th>zero</th>\n      <th>zeus</th>\n      <th>zf</th>\n      <th>zheng</th>\n      <th>zillow</th>\n      <th>zogsports</th>\n      <th>zones</th>\n      <th>zoom</th>\n      <th>zuckerberg</th>\n      <th>zurich</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 8749 columns</p>\n</div>",
      "text/plain": "   00  000  02115  03  0305  0356  04  062  06366  08  ...  zero  zeus  zf  \\\n0   0    0      0   0     0     0   0    0      0   0  ...     0     0   0   \n1   0    0      0   0     0     0   0    0      0   0  ...     0     0   0   \n2   0    0      0   0     0     0   0    0      0   0  ...     0     0   0   \n3   0    0      0   0     0     0   0    0      0   0  ...     0     0   0   \n4   0    0      0   0     0     0   0    0      0   0  ...     0     0   0   \n\n   zheng  zillow  zogsports  zones  zoom  zuckerberg  zurich  \n0      0       0          0      0     0           0       0  \n1      0       0          0      0     0           0       0  \n2      0       0          0      0     0           0       0  \n3      1       0          0      0     0           0       0  \n4      0       0          0      0     0           0       0  \n\n[5 rows x 8749 columns]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
    "## 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "from collections import Counter\n",
    "def count(docs):\n",
    "\n",
    "        word_counts = Counter()\n",
    "        appears_in = Counter()\n",
    "        \n",
    "        total_docs = len(docs)\n",
    "\n",
    "        for doc in docs:\n",
    "            word_counts.update(doc)\n",
    "            appears_in.update(set(doc))\n",
    "\n",
    "        temp = zip(word_counts.keys(), word_counts.values())\n",
    "        \n",
    "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "        total = wc['count'].sum()\n",
    "\n",
    "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "        wc = wc.sort_values(by='rank')\n",
    "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "        t2 = zip(appears_in.keys(), appears_in.values())\n",
    "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "        wc = ac.merge(wc, on='word')\n",
    "\n",
    "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "        return wc.sort_values(by='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = count(df['description_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>appears_in</th>\n      <th>count</th>\n      <th>rank</th>\n      <th>pct_total</th>\n      <th>cul_pct_total</th>\n      <th>appears_in_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>data</td>\n      <td>421</td>\n      <td>3914</td>\n      <td>1.0</td>\n      <td>0.028902</td>\n      <td>0.028902</td>\n      <td>0.988263</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td></td>\n      <td>361</td>\n      <td>2225</td>\n      <td>2.0</td>\n      <td>0.016430</td>\n      <td>0.045332</td>\n      <td>0.847418</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>experience</td>\n      <td>401</td>\n      <td>1668</td>\n      <td>3.0</td>\n      <td>0.012317</td>\n      <td>0.057649</td>\n      <td>0.941315</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>work</td>\n      <td>350</td>\n      <td>1063</td>\n      <td>4.0</td>\n      <td>0.007849</td>\n      <td>0.065498</td>\n      <td>0.821596</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>business</td>\n      <td>303</td>\n      <td>1052</td>\n      <td>5.0</td>\n      <td>0.007768</td>\n      <td>0.073266</td>\n      <td>0.711268</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>team</td>\n      <td>325</td>\n      <td>786</td>\n      <td>6.0</td>\n      <td>0.005804</td>\n      <td>0.079070</td>\n      <td>0.762911</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>learning</td>\n      <td>271</td>\n      <td>675</td>\n      <td>7.0</td>\n      <td>0.004984</td>\n      <td>0.084055</td>\n      <td>0.636150</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>machine</td>\n      <td>271</td>\n      <td>674</td>\n      <td>8.0</td>\n      <td>0.004977</td>\n      <td>0.089031</td>\n      <td>0.636150</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>science</td>\n      <td>246</td>\n      <td>600</td>\n      <td>9.0</td>\n      <td>0.004431</td>\n      <td>0.093462</td>\n      <td>0.577465</td>\n    </tr>\n    <tr>\n      <th>539</th>\n      <td>analytics</td>\n      <td>212</td>\n      <td>555</td>\n      <td>10.0</td>\n      <td>0.004098</td>\n      <td>0.097560</td>\n      <td>0.497653</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>statistical</td>\n      <td>260</td>\n      <td>542</td>\n      <td>11.0</td>\n      <td>0.004002</td>\n      <td>0.101562</td>\n      <td>0.610329</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>new</td>\n      <td>242</td>\n      <td>519</td>\n      <td>12.0</td>\n      <td>0.003832</td>\n      <td>0.105395</td>\n      <td>0.568075</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>ability</td>\n      <td>237</td>\n      <td>502</td>\n      <td>13.0</td>\n      <td>0.003707</td>\n      <td>0.109102</td>\n      <td>0.556338</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>product</td>\n      <td>188</td>\n      <td>479</td>\n      <td>14.0</td>\n      <td>0.003537</td>\n      <td>0.112639</td>\n      <td>0.441315</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>skills</td>\n      <td>255</td>\n      <td>453</td>\n      <td>15.0</td>\n      <td>0.003345</td>\n      <td>0.115984</td>\n      <td>0.598592</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>help</td>\n      <td>234</td>\n      <td>448</td>\n      <td>16.0</td>\n      <td>0.003308</td>\n      <td>0.119292</td>\n      <td>0.549296</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>working</td>\n      <td>246</td>\n      <td>441</td>\n      <td>17.0</td>\n      <td>0.003256</td>\n      <td>0.122548</td>\n      <td>0.577465</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>models</td>\n      <td>206</td>\n      <td>437</td>\n      <td>18.0</td>\n      <td>0.003227</td>\n      <td>0.125775</td>\n      <td>0.483568</td>\n    </tr>\n    <tr>\n      <th>608</th>\n      <td>analysis</td>\n      <td>243</td>\n      <td>431</td>\n      <td>19.0</td>\n      <td>0.003183</td>\n      <td>0.128958</td>\n      <td>0.570423</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>scientist</td>\n      <td>253</td>\n      <td>409</td>\n      <td>20.0</td>\n      <td>0.003020</td>\n      <td>0.131978</td>\n      <td>0.593897</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "            word  appears_in  count  rank  pct_total  cul_pct_total  \\\n5           data         421   3914   1.0   0.028902       0.028902   \n121                      361   2225   2.0   0.016430       0.045332   \n16    experience         401   1668   3.0   0.012317       0.057649   \n45          work         350   1063   4.0   0.007849       0.065498   \n199     business         303   1052   5.0   0.007768       0.073266   \n204         team         325    786   6.0   0.005804       0.079070   \n39      learning         271    675   7.0   0.004984       0.084055   \n65       machine         271    674   8.0   0.004977       0.089031   \n72       science         246    600   9.0   0.004431       0.093462   \n539    analytics         212    555  10.0   0.004098       0.097560   \n358  statistical         260    542  11.0   0.004002       0.101562   \n306          new         242    519  12.0   0.003832       0.105395   \n67       ability         237    502  13.0   0.003707       0.109102   \n140      product         188    479  14.0   0.003537       0.112639   \n60        skills         255    453  15.0   0.003345       0.115984   \n264         help         234    448  16.0   0.003308       0.119292   \n144      working         246    441  17.0   0.003256       0.122548   \n13        models         206    437  18.0   0.003227       0.125775   \n608     analysis         243    431  19.0   0.003183       0.128958   \n150    scientist         253    409  20.0   0.003020       0.131978   \n\n     appears_in_pct  \n5          0.988263  \n121        0.847418  \n16         0.941315  \n45         0.821596  \n199        0.711268  \n204        0.762911  \n39         0.636150  \n65         0.636150  \n72         0.577465  \n539        0.497653  \n358        0.610329  \n306        0.568075  \n67         0.556338  \n140        0.441315  \n60         0.598592  \n264        0.549296  \n144        0.577465  \n13         0.483568  \n608        0.570423  \n150        0.593897  "
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
    "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>02115</th>\n      <th>03</th>\n      <th>0305</th>\n      <th>0356</th>\n      <th>04</th>\n      <th>062</th>\n      <th>06366</th>\n      <th>08</th>\n      <th>...</th>\n      <th>zero</th>\n      <th>zeus</th>\n      <th>zf</th>\n      <th>zheng</th>\n      <th>zillow</th>\n      <th>zogsports</th>\n      <th>zones</th>\n      <th>zoom</th>\n      <th>zuckerberg</th>\n      <th>zurich</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.106253</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 8495 columns</p>\n</div>",
      "text/plain": "    00  000  02115   03  0305  0356   04  062  06366   08  ...  zero  zeus  \\\n0  0.0  0.0    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  ...   0.0   0.0   \n1  0.0  0.0    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  ...   0.0   0.0   \n2  0.0  0.0    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  ...   0.0   0.0   \n3  0.0  0.0    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  ...   0.0   0.0   \n4  0.0  0.0    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  ...   0.0   0.0   \n\n    zf     zheng  zillow  zogsports  zones  zoom  zuckerberg  zurich  \n0  0.0  0.000000     0.0        0.0    0.0   0.0         0.0     0.0  \n1  0.0  0.000000     0.0        0.0    0.0   0.0         0.0     0.0  \n2  0.0  0.000000     0.0        0.0    0.0   0.0         0.0     0.0  \n3  0.0  0.106253     0.0        0.0    0.0   0.0         0.0     0.0  \n4  0.0  0.000000     0.0        0.0    0.0   0.0         0.0     0.0  \n\n[5 rows x 8495 columns]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate vectorizer object\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Create a vocabulary and get word counts per document\n",
    "dtm2 = tfidf.fit_transform(df['description'])\n",
    "\n",
    "# Print word counts\n",
    "\n",
    "# Get feature names to use as dataframe column headers\n",
    "dtm2 = pd.DataFrame(dtm2.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0      0.072510\n1      0.016550\n2      0.053281\n3      0.000000\n4      0.000000\n5      0.020574\n6      0.022355\n7      0.022126\n8      0.019714\n9      0.013731\n10     0.019589\n11     0.040895\n12     0.028341\n13     0.022355\n14     0.015411\n15     0.015367\n16     0.013830\n17     0.030067\n18     0.022126\n19     0.044793\n20     0.037133\n21     0.000000\n22     0.025196\n23     0.000000\n24     0.000000\n25     0.032141\n26     0.026130\n27     0.014314\n28     0.028341\n29     0.000000\n         ...   \n396    0.018262\n397    0.019111\n398    0.018300\n399    0.022307\n400    0.000000\n401    0.020247\n402    0.023370\n403    0.039574\n404    0.014388\n405    0.000000\n406    0.018041\n407    0.022123\n408    0.030367\n409    0.013064\n410    0.028872\n411    0.008073\n412    0.014444\n413    0.028865\n414    0.014495\n415    0.030060\n416    0.016266\n417    0.021104\n418    0.029947\n419    0.027645\n420    0.018536\n421    0.011734\n422    0.000000\n423    0.039643\n424    0.080851\n425    0.013036\nName: python, Length: 426, dtype: float64"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2['python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n         metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
    "nn.fit(dtm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0.        , 1.20736794, 1.21131293, 1.2241482 , 1.23390372]]),\n array([[ 36, 185, 201, 336, 353]]))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.kneighbors([dtm2.iloc[36]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
    "## Stretch Goals\n",
    "\n",
    " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
    " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python 3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}